\chapter{Introduction}

general flow: introduction of origin of nuclear theory -> structure of nucleus and nuclear matter -> problems of nuclear theory (how nuclei work, astrophysical applications (origin of elements), nucleus as a laboratory in the search for BSM physics, applications in national security, energy, medicine) -> explanation of past struggles with computational difficulty (phenom models) -> explanation that these models have low predictive power outside the domain in which they were fit -> need for better nuclear theory calculations in light of experimental advances in low-energy nuclear physics (FRIB) and astrophysics (LIGO results suggesting site of r-process) -> rapid expansion of range of calculations from first principles in recent years due to improvements in computational hardware and development of new computational methods -> focus on modern ab-initio methods (NCSM, IM-SRG, lattice QCD), RG methods, EFT methods -> use of these methods is state of the art in low-energy nuclear theory -> study of their correct, optimal application to calculations is an open problem


Nuclear theory, which attempts to model the atomic nucleus and more generally nuclear matter, has been studied since the discovery of the neutron in 1932 (source? Dainton thesis). Nuclear matter is made of positively-charged protons and uncharged neutrons, which are generally referred to as nucleons. Nucleons interact primarily through the strong interaction, which ensures that stable and unstable nuclei are bound. Nuclear theory seeks to answer open questions in four areas: how doe nuclei work, what are the properties of nuclear matter in astrophysical systems, what can be learned about beyond standard model (BSM) physics from the nucleus, and how can more accurate models of nuclear systems be leveraged in a variety of applications, ranging from national security and energy to medicine.

Nuclear theory faces two major challenges when trying to model systems of nucleons at low energies. The first is that, even for modest nucleus sizes, this is a quantum many-body problem. Quantum many-body theory is relevant to many different fields, including quantum chemistry and condensed matter theory. Many-body problems quickly become intractable when approached naively due to the combinatorial growth of the size of the problem with respect to the number of particles. The second challenge of nuclear theory is that, since the strong interaction is an interaction between partons (quarks and gluons which make up nucleons), a closed form of it between nucleons does not exist. These challenges forced nuclear physics in the past to rely on phenomenological models, both for the form of the strong interaction and the treatment of the many-body problem. These models were typically fit to experimental data for certain nuclei and used to predict the properties of nearby nuclei. However, their predictive ability did not extend far outside the domain in which they were fit, limiting their range of applicability.

Recent advancements in experimental nuclear physics, with facilities like the Facility for Rare Isotope Beams (FRIB) and the Laser Interferometer Gravitational-Wave Observatory (LIGO), have generated demand for more accurate calculations of already-calculated nuclear observables (need to introduce this concept somewhere) as well as new calculations of untouched territories with larger, more exotic nuclei. Moreover, accurate nuclear models are essential to understanding dense stars, supernovae, and certain astrophysical events, such as the collisions of neutron stars, which are hypothesized to be a location for the synthesis of heavy elements, such as gold and silver. Improved nuclear models are also essential in predicting the rate of certain nuclear decays, such as neutrinoless double-beta decay, which is currently being searched for by many experiments to answer whether neutrinos are Majorana particles, which means they are their own anti-particle, or not.

In recent years, the improvements in computational hardware and the development of new computational methods has brought about a rapid expansion in the range of nuclear isotopes able to be modeled via ab-initio calculations. The improvements in computational hardware have come through the development and proliferation of high-throughput devices (such as GPUs) and the assembly of highly parallel systems. Current trends suggest that a machine with exa-FLOP (floating-operations per second) throughput will exist by 2020 and such systems will be commonplace soon after. Much work is being done to ensure that the processing, networking, and power-consumption of these systems will continue to scale as it has for the past two decades. 

The computational methods used in modern low-energy nuclear theory come in three classes: interaction models, renormalization group (RG) methods, and effective field theory (EFT) methods. Modern interaction models, like no-core shell model (NCSM) and lattice quantum chromodynamics (lattice QCD), seek to model nuclei from first principles and work in conjunction with RG and EFT methods to make these calculations feasible. RG methods modify interactions to match the resolution relevant to the problem at hand, which for low-energy nuclear physics means framing the problem in terms of low-energy nucleon-nucleon interactions, as opposed to the parton-parton interaction of the strong force. EFT methods offer systematically improvable, model-independent ordering of components of some interaction that allows for importance truncation and uncertainty quantification. These methods have already been used to offer both theoretical predictions that improve on previous calculations and theoretical predictions for nuclei that were unable to be modeled previously. The use of these methods in low-energy nuclear calculations is the state of the art. As a result, questions regarding their application are open problems and the focus of a lot of research.


\section{Similarity Renormalization Group}

The similarity renormalization group (SRG), whose use in nuclear physics was initially explored at OSU, is one method of reducing the computational complexity of low-energy nuclear calculations. The idea behind it is to continuously unitarily transform the operator of interest (for example, the Hamiltonian) into a simpler form. This simpler form is chosen to allow the large basis to be truncated without affecting the operator eigenvalues, which are essential to the truncated operator's utility in later calculations.

\subsection{Jacobi Coordinates}

\subsection{Harmonic Oscillator States with Proper Symmetry}
