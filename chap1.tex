\chapter{Introduction}

% general flow: introduction of origin of nuclear theory -> structure of nucleus and nuclear matter -> problems of nuclear theory (how nuclei work, astrophysical applications (origin of elements), nucleus as a laboratory in the search for BSM physics, applications in national security, energy, medicine) -> explanation of past struggles with computational difficulty (phenom models) -> explanation that these models have low predictive power outside the domain in which they were fit -> need for better nuclear theory calculations in light of experimental advances in low-energy nuclear physics (FRIB) and astrophysics (LIGO results suggesting site of r-process) -> rapid expansion of range of calculations from first principles in recent years due to improvements in computational hardware and development of new computational methods -> focus on modern ab-initio methods (NCSM, IM-SRG, lattice QCD), RG methods, EFT methods -> use of these methods is state of the art in low-energy nuclear theory -> study of their correct, optimal application to calculations is an open problem


Nuclear theory, which attempts to model the atomic nucleus and more generally nuclear matter, has been studied since the discovery of the neutron in 1932 (source? Dainton thesis). Nuclear matter is made of positively-charged protons and uncharged neutrons, which are generally referred to as nucleons. Nucleons interact primarily through the strong interaction, which ensures that stable and unstable nuclei are bound. Nuclear theory seeks to answer open questions in four areas: how do nuclei work, what are the properties of nuclear matter in astrophysical systems, what can be learned about beyond standard model (BSM) physics from the nucleus, and how can more accurate models of nuclear systems be leveraged in a variety of applications, ranging from national security and energy to medicine.

Nuclear theory faces two major challenges when trying to model systems of nucleons at low energies. The first is that, even for modest nucleus sizes, this is a quantum many-body problem. Quantum many-body theory is relevant to many different fields, including quantum chemistry and condensed matter theory. Many-body problems quickly become intractable when approached naively due to the combinatorial growth of the size of the problem with respect to the number of particles. The second challenge of nuclear theory is that, since the strong interaction is an interaction between partons (quarks and gluons which make up nucleons), a closed form of it between nucleons does not exist. These challenges forced nuclear physics in the past to rely on phenomenological models, both for the form of the strong interaction and the treatment of the many-body problem. These models were typically fit to experimental data for certain nuclei and used to predict the properties of nearby nuclei. However, their predictive ability did not extend far outside the domain in which they were fit, limiting their range of applicability.

Recent advancements in experimental nuclear physics, with facilities like the Facility for Rare Isotope Beams (FRIB) and the Laser Interferometer Gravitational-Wave Observatory (LIGO), have generated demand for more accurate calculations of already-calculated nuclear observables (need to introduce this concept somewhere) as well as new calculations of untouched territories with larger, more exotic nuclei. Moreover, accurate nuclear models are essential to understanding dense stars, supernovae, and certain astrophysical events, such as the collisions of neutron stars, which are hypothesized to be a location for the synthesis of heavy elements, such as gold and silver. Improved nuclear models are also essential in predicting the rate of certain nuclear decays, such as neutrino-less double-beta decay, which is currently being searched for by many experiments to answer whether neutrinos are Majorana particles, which means they are their own anti-particle, or not.

In recent years, the improvements in computational hardware and the development of new computational methods has brought about a rapid expansion in the range of nuclear isotopes able to be modeled via ab-initio calculations, or calculations from first principles (explain QCD?). The improvements in computational hardware have come through the development and proliferation of high-throughput devices (such as GPUs) and the assembly of highly parallel systems. Current trends suggest that a machine with exa-FLOP (floating-operations per second) throughput will exist by 2020 and such systems will be commonplace soon after. Much work is being done to ensure that the processing, networking, and power-consumption of these systems will continue to scale as it has for the past two decades. 

The computational methods used in modern low-energy nuclear theory come in three classes: interaction models, renormalization group (RG) methods, and effective field theory (EFT) methods. Modern interaction models, like no-core shell model (NCSM) and lattice quantum chromodynamics (lattice QCD), seek to model nuclei from first principles and work in conjunction with RG and EFT methods to make these calculations feasible. RG methods modify interactions to match the resolution relevant to the problem at hand, which for low-energy nuclear physics means framing the problem in terms of low-energy nucleon-nucleon interactions, as opposed to the parton-parton interaction of the strong force. EFT methods offer systematically improvable, model-independent ordering of components of some interaction that allows for importance truncation and uncertainty quantification. These methods have already been used to offer both theoretical predictions that improve on previous calculations and theoretical predictions for nuclei that were unable to be modeled previously. The use of these methods in low-energy nuclear calculations is the state of the art. As a result, questions regarding their application are open problems and the focus of a lot of research.

The similarity renormalization group method (SRG) is an interaction softening method from the class of RG methods whose use in nuclear theory was first explored in a 1-dimensional setting at OSU by Eric Jurgenson in 2009 (citation?). It is a class of continuous unitary transformations that decouple large off-diagonal matrix elements in the interaction Hamiltonian, softening the potential as a result. The evolution of the Hamiltonian to a decoupled form allows a truncated subspace of the original basis to be used in later calculations without affecting the lowest energy eigenvalues which are the aspect of the operator that determine the calculated observables. This basis truncation offers a significant reduction in the size of the problem. SRG is frequently used in modern nuclear theory calculations to soften interactions and extend the range of certain calculations to larger systems. In our work, we return to a simple 1-dimensional setting to study features of SRG and seek to understand how to optimize its use in many-body calculations.

\section{Contributions}

Our main contributions are:
\begin{itemize}
    \item{We have developed an open-source Python library with easy to use abstractions for the SRG method for use by others to test SRG in a simple setting.}
    \item{We verify the results published in the 2009 Jurgenson paper, suggesting the correctness of our implementation. We also provide a fairly comprehensive suite of tests for the library, verifying that it fails gracefully when misused and correctly calculates the results simple analytically solvable cases when used correctly.}
    \item{We explore some alternative transformation generators and seek to quantify their performance relative to the standard generator that is currently used. We discuss the results of these calculations and offer some preliminary analysis.}
\end{itemize}

\section{Outline}

The rest of the thesis is as follows:
\begin{itemize}
    \item{In chapter 2, we review the matrix representation of quantum operators. We then discuss the details of two different bases used, relative Jacobi momentum coordinates and 1-dimensional symmetrized harmonic oscillator states. We then discuss the details of the SRG method and explain the need for a careful revisit of SRG in a 1-dimensional setting.}
    \item{In chapter 3, we explain the design of the Python library. Much of the effort on this project went into making the library design logical and simple-to-use, so this chapter will explain the abstractions made and the reasoning behind those decisions.}
    \item{In chapter 4, we discuss ???}
    \item{In chapter 5, we discuss ???}
\end{itemize}

